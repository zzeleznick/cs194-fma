---
layout: post
title: Building a Pinhole Camera
subtitle:
images: site.images
---

h2(anchor#main). %(octicon octicon-link)I. Introduction%


<div class="pure-g">
 <div class="pure-u-2-3 pure-u-md-2-3">
    <notextile>
    In this project we designed a pinhole camera ("camera obscura"),
    which is essentially a dark box with a pinhole on one face,
    and a screen on the opposite side.
    Light reflecting off a focal object is directed through the pinhole onto the screen,
    and an inverted image of the object is formed.
    The caveat is that it is hard to see projected image with the naked eye alone.
    In order to see the image clearly, we will use a digital camera with a long exposure time (15-30 seconds)
    attached to the pinhole camera.
    </notextile>
</div>
 <div class="pure-u-1-3 pure-u-md-1-3">
    <img src="../../../images/p2/pinhole.jpg" style="padding: 0 0 10px 40px;">
</div>
</div>

<h2>  II. Implementation </h2>
<notextile>
For our pinhole camera, we used a shoebox for our camera body,
hole-punched index cards for our aperatures,
and a DSLR camera capturing our projected images.
We noticed that with a larger opening, more photons were able to pass through our pinhole and project onto the opposite face.
Yet, the clarity of the image was greatly reduced due to the scattering effect of light.
Exposure time was a second parameter we could adjust, and with the smaller pinhole keeping the exposure time constant
resulted in much darker, but sharper images.
</notextile>
<br>
<div class="pure-g">
{% for image in site.p2_design offset:0%}
    <div class="pure-u-1-3 pure-u-md-1-3 center"> !{{ site.github_path | escape  }}{{site.p2_path}}{{image.ref}}({{image.title}})!:http://zzeleznick.github.io/cs194-fma/{{site.p2_path}}{{image.ref}}
         {{image.title}}
       <br><br>
    </div>
{% endfor %}
</div>

<h2> III. Results </h2>
<notextile>
We used 100mm, 3cm, and 5cm diameters for our apeture sizes, and 30s, 30s, 15s shutter speeds respectively.
The best shot we took used the medium size pinhole of 3cm diameter at an exposure time of 30 seconds.
However, we were limited to a max of 30 second shutter speed by our camera,
so we weren't able to increase exposure time to adjust for low-light settings.
</notextile>
<br>
<div class="pure-g">
{% for image in site.p2_uni offset:0%}
    <div class="pure-u-1-3 pure-u-md-1-3 center"> !{{ site.github_path | escape  }}{{site.p2_path}}{{image.ref}}({{image.title}})!:http://zzeleznick.github.io/cs194-fma/{{site.p2_path}}{{image.ref}}
         {{image.title}}
    <br>
    </div>
{% endfor %}
{% for image in site.p2_arch offset:0%}
    <div class="pure-u-1-3 pure-u-md-1-3 center"> !{{ site.github_path | escape  }}{{site.p2_path}}{{image.ref}}({{image.title}})!:http://zzeleznick.github.io/cs194-fma/{{site.p2_path}}{{image.ref}}
         {{image.title}}
       <br>
    </div>
{% endfor %}
{% for image in site.p2_stairs offset:0%}
    <div class="pure-u-1-3 pure-u-md-1-3 center"> !{{ site.github_path | escape  }}{{site.p2_path}}{{image.ref}}({{image.title}})!:http://zzeleznick.github.io/cs194-fma/{{site.p2_path}}{{image.ref}}
         {{image.title}}
       <br>
    </div>
{% endfor %}
</div>


<h2> IV. Light Painting </h2>
<notextile>
Following the warp of our target image, we can combine the base and rectified image to
produce an image mosiac. The base and target image will have an overlapping region encompassing 40-70%
of the image mosiac. In order to reduce artifacts, we can use a weighted linear blend (alpha, 1-alpha) such that as we move away from
one image to the next, our weight (alpha) decreases. Other methods of blending (such as Laplacian or Poisson) could
have better results, but a linear blend produced comparable results.
</notextile>
<br>

<h2> V. Automatic Image Stitching Results </h2>
<notextile>
Right off the gun, here are the results from automatic stitching via Harris Corner Detection, Adaptive Non-Maximal
Suppression, Descriptor Matching, and RANSAC. These methods will be explained in more detail shortly.
</notextile>
<br>
<div class="pure-g">
{% for image in site.p8_autos offset:0%}
    <div class="pure-u-1-1 pure-u-md-1-1 center"> !{{ site.github_path | escape  }}{{site.p8_path}}{{image.ref}}({{image.title}})!:http://zzeleznick.github.io/cs194-fma/{{site.p8_path}}{{image.ref}}
         {{image.title}}
       <br><br>
    </div>
{% endfor %}
</div>
<br>

<h2> VII. Image Originals </h2>
<div class="pure-g">
{% for image in site.p7_originals offset:0%}
    <div class="pure-u-1-3 pure-u-md-1-3 center"> !{{ site.github_path | escape  }}{{site.p7_path}}{{image.ref}}({{image.title}})!:http://zzeleznick.github.io/cs194-fma/{{site.p7_path}}{{image.ref}}
        {{image.title}}
       <br><br>
    </div>
{% endfor %}
</div>

<h2 class="center"> VIII. Summary </h2>

<notextile>
Overall, this project was quite challenging, but I felt huge a wave of relief once I was finally able to
generate the photo mosiac. I encountered significant delays mapping the new coordinates in the canvas back
to the original image, namely due to determining the proper bounds of the canvas and pixels coordinates
to pull from. Shifting conventions between labeling x,y and row, column order from human and numpy
coordinates, and then accounting for top vs bottom offsets with new coordinate axes systems added
additional complexity. I thought the final product was a great achievement, although the built-in
panorama feature on the iPhone is superior. If I revisit this project,
I want to generate an explorable photosphere.
</notextile>
<br>

<comment>
<h3 class="center"> Bells & Whistles </h3>
1) Presidential Morph Video

2) Facial Gender Bender

3) Participation in Class Morph
</comment>

1. <a href='https://cseweb.ucsd.edu/classes/wi07/cse252a/homography_estimation/homography_estimation.pdf' target="_blank"> Homography Estimation </a>
<br>
<h5 class='center'>  - Zachary Zeleznick </h5>